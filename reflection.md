# Reflection on Secure User Model and CI/CD Assignment

For this assignment, I built a FastAPI application focused on implementing a secure user model and connecting it all the way through to testing and deployment with Docker and GitHub Actions.

The first major step was designing the SQLAlchemy `User` model. I added fields for `id`, `username`, `email`, `password_hash`, and `created_at`, and enforced uniqueness on both `username` and `email`. One detail I paid attention to was using a server-side default for `created_at` so the timestamp is generated by the database. On top of the ORM model, I created Pydantic schemas `UserCreate` and `UserRead` to clearly separate incoming user data (including the plain-text password) from the data that is returned to clients (which intentionally omits any password-related fields). This separation helped make the API safer and clearer to use.

Implementing secure password handling was another core part of the assignment. Instead of storing plain-text passwords, I wrote helper functions that hash passwords using PBKDF2-HMAC-SHA256. The `hash_password` function produces a string that encodes the iterations, salt, and derived key, and `verify_password` recomputes the hash and compares it using a constant-time comparison. This was a good reminder that password hashing needs both a strong algorithm and careful handling of salts and comparisons. Moving away from the initial bcrypt configuration issues and implementing PBKDF2 manually also helped me better understand what is actually happening under the hood.

Once the basic model and hashing were in place, I wrote unit tests and integration tests with pytest. The unit tests focus on the hashing helpers and Pydantic schemas, verifying that hashes differ from the original password, that verification works for correct and incorrect passwords, and that invalid emails are rejected early. The integration tests spin up a real Postgres database and exercise the `/users/` API endpoints using FastAPI's TestClient. They validate successful user creation, uniqueness enforcement for username and email, and proper HTTP error codes for invalid input. Seeing all the tests pass gave me confidence that the core pieces of the application behave as expected and that the database schema, validation, and API layer are wired together correctly.

The final part of the assignment was configuring CI/CD. I wrote a GitHub Actions workflow that starts a Postgres service, sets the `DATABASE_URL` environment variable for the tests, installs dependencies, and runs the full test suite on every push and pull request to the `main` branch. On successful pushes to `main`, the pipeline logs into Docker Hub using encrypted repository secrets, builds the Docker image from the Dockerfile, and pushes the `latest` tag to my Docker Hub repository. Locally, I also wired up a `docker-compose.yml` file that runs both the Postgres database and the FastAPI application container, so I can reproduce the same environment that CI uses.

Overall, the main challenges were getting the database connection working consistently between local development, tests, and the CI environment, and making sure the Pydantic configuration matched Pydantic v2’s `from_attributes` behavior instead of the older `orm_mode` pattern. Working through these details helped me better understand how the API, database, testing, Docker, and GitHub Actions all fit together into a full pipeline—from writing the model to having a tested and containerized application automatically deployed to Docker Hub. This assignment gave me a much clearer picture of what a realistic backend service with proper security and CI/CD looks like.

In the next step (Module 11), I extended the project by adding a Calculation model and related Pydantic schemas. 
I used an enum to restrict the operation type and a validator to block division by zero. I also implemented a simple 
factory pattern so that each calculation type (add, subtract, multiply, divide) is handled by its own strategy class. 
Writing unit and integration tests for the new model and factory made it clear how the data layer, validation, and 
business logic can be tested independently while still fitting into the same CI/CD pipeline I built in the previous module.
